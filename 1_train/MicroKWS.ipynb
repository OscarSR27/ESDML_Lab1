{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "729ef4ca",
   "metadata": {},
   "source": [
    "# MicroKWS Training Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2659464b",
   "metadata": {},
   "source": [
    "This notebook should give an introduction on the required procedure to design, train and quantize a small machine learning model using the Tensorflow Lite and the Keras API. The application example is a keyword-spotting (KWS) task which should ideally be suitable to run on an energy efficent device e.g. a small microcontroller platform (see next lab).\n",
    "\n",
    "In the following a step by step guide is provided. Please follow the notebooks contents sequentially by executiong one cell after each other while inspecting the used python code as well as the program outputs printed to the screen. From time to time theoretical tasks are going to be introduced. Please try to answer them **without changing the contents of the previous cells**. At the end of this document, there is a Programming Challenge which has to be solved alongside with the previous theoretical questions to pass this lab assignment. As this challenge involves some programming, it is recommended to duplicate this notebook before starting to play around with the code in the allowed cells.\n",
    "\n",
    "If you have never heard of *Jupyter Notebooks* before, please first have a look at the Setup section in the Lab Manual and check out https://docs.jupyter.org/en/latest/start/index.html for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b589f4",
   "metadata": {},
   "source": [
    "### Disclaimer\n",
    "\n",
    "This tutorial is inpired by the contents of: https://github.com/ARM-software/ML-examples/tree/main/tflu-kws-cortex-m/Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb549695",
   "metadata": {},
   "source": [
    "## 0. Install software"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d34d8b",
   "metadata": {},
   "source": [
    "The following steps should ideally done before launching this Jupyter notebook! (See `README.md`!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a576794",
   "metadata": {},
   "source": [
    "**1. Clone repository**\n",
    "\n",
    "```\n",
    "git clone git@gitlab.lrz.de:de-tum-ei-eda-esl/ESD4ML/micro-kws.git\n",
    "```\n",
    "\n",
    "\n",
    "**2. Create virtual python environment**\n",
    "\n",
    "```\n",
    "virtualenv -p python3.8 venv\n",
    "```\n",
    "\n",
    "**3. Enter virtual python environment**\n",
    "\n",
    "```\n",
    "source venv/bin/activate\n",
    "```\n",
    "\n",
    "**4. Enter directory**\n",
    "\n",
    "```\n",
    "cd micro-kws/1_train\n",
    "```\n",
    "\n",
    "**5. Install python packages into environment**\n",
    "\n",
    "```\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "**6. Start jupyter notebook**\n",
    "    \n",
    "```\n",
    "jupyter notebook Flow.ipynb\n",
    "```\n",
    "\n",
    "  If using a remote host, append: ` --no-browser --ip 0.0.0.0 --port XXXX` (where XXXX should be a number greater than 1000)\n",
    "  \n",
    "  If you experience warnings it might help to use ` --NotebookApp.iopub_msg_rate_limit=1.0e10  --NotebookApp.iopub_data_rate_limit=1.0e10`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1c756a-437f-4cae-aea3-2ebeea440e37",
   "metadata": {},
   "source": [
    "The following \"IPython magic\" allows editing python files without restarting the Jupyter kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9b875711-4234-4a08-8ace-fc969763ef39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f568a9df",
   "metadata": {},
   "source": [
    "## 1. Python imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4ef7a0",
   "metadata": {},
   "source": [
    "Python builtin dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e9313e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\"  # Reduce verbosity\n",
    "import argparse\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed9e483",
   "metadata": {},
   "source": [
    "Third party dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8b8d8efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30dd0c5",
   "metadata": {},
   "source": [
    "Jupyter specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7c356b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import FileLink"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c173d714",
   "metadata": {},
   "source": [
    "Helper scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b7d8c8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import data\n",
    "import models\n",
    "from test import get_accuracy, get_confusion_matrix\n",
    "from test_tflite import tflite_test\n",
    "from estimate import load_model, estimate_model_macs, estimate_model_rom, estimate_model_ram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b719775-6edf-4972-800c-c7bbc979294e",
   "metadata": {},
   "source": [
    "Import student code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e996e0fb-95a2-4fc0-87cb-d0b4133dd5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from student.metrics import get_student_metrics\n",
    "from student.callbacks import get_student_callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a864db",
   "metadata": {},
   "source": [
    "## 2. Define training parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8132f6ea",
   "metadata": {},
   "source": [
    "In this section the hyperparameters for the model and training procedure are defined. Feel free to read through the code line by line as the options should be all documented well. You are NOT supposed to change any parameters except the following:\n",
    "\n",
    "- `FLAGS.model_name`: This will be used as the base filename when writing the converted model to the disk. Change this from `\"kws_model_xs\"` to `kws_model_student` when starting the Programming challenge.\n",
    "- `FLAGS.data_dir`: May be changed to a persitent directory to keep the downloaded dataset between reboots if working on a personal machine. If working on a chair computer, feel free to change this to `/usr/local/labs/ESD4ML/current/common/data/speech_commands_v0.02` to skip the download procedure.\n",
    "- `FLAGS.wanted_words`: should be changed to the set of keywords which was assigned to your group **after** answering the theoretical questions on the default pair of keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c3210afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS = argparse.Namespace()\n",
    "\n",
    "# Overwrite the model name provided by keras with a custom one\n",
    "FLAGS.model_name = \"micro_model_student\"\n",
    "\n",
    "# Location of speech training data archive on the web.\n",
    "FLAGS.data_url = \"http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz\"\n",
    "\n",
    "# Where to download the speech training data to.\n",
    "try:\n",
    "    login = os.getlogin()\n",
    "except:\n",
    "    login = \"unknown\"\n",
    "FLAGS.data_dir = os.getenv(\n",
    "    \"SPEECH_COMMANDS_DIR\",\n",
    "    default=os.path.join(tempfile.gettempdir(), login, \"speech_dataset\"),\n",
    ")\n",
    "\n",
    "# Words to use (others will be added to an unknown label)\n",
    "FLAGS.wanted_words = \"yes,no\"\n",
    "\n",
    "# Total number of classifiations labels (wanted_words + silence + unknown)\n",
    "FLAGS.num_classes = len(FLAGS.wanted_words.split(\",\")) + 2\n",
    "\n",
    "# How loud the background noise should be, between 0 and 1.\n",
    "FLAGS.background_volume = 0.1\n",
    "\n",
    "# How many of the training samples have background noise mixed in.\n",
    "FLAGS.background_frequency = 0.8\n",
    "\n",
    "# How much of the training data should be silence.\n",
    "FLAGS.silence_percentage = 100.0 / FLAGS.num_classes\n",
    "\n",
    "# How much of the training data should be unknown words\n",
    "FLAGS.unknown_percentage = 100.0 / FLAGS.num_classes\n",
    "\n",
    "# Range to randomly shift the training audio by in time.\n",
    "FLAGS.time_shift_ms = 100.0\n",
    "\n",
    "# What percentage of wavs to use as a test set.\n",
    "FLAGS.testing_percentage = 10\n",
    "\n",
    "# What percentage of wavs to use as a validation set.\n",
    "FLAGS.validation_percentage = 10\n",
    "\n",
    "# Expected sample rate of the wavs\n",
    "FLAGS.sample_rate = 16000\n",
    "\n",
    "# Expected duration in milliseconds of the wavs\n",
    "FLAGS.clip_duration_ms = 1000\n",
    "\n",
    "# How long each spectrogram timeslice is\n",
    "FLAGS.window_size_ms = 30.0\n",
    "\n",
    "# How long each spectrogram timeslice is\n",
    "FLAGS.window_stride_ms = 20.0\n",
    "\n",
    "# How many bins to use for the MFCC fingerprint\n",
    "FLAGS.dct_coefficient_count = 40\n",
    "\n",
    "# How many training loops to run\n",
    "FLAGS.how_many_training_steps = \"12000,3000\"\n",
    "\n",
    "# How often to evaluate the training results.\n",
    "FLAGS.eval_step_interval = 400\n",
    "\n",
    "# How large a learning rate to use when training.\n",
    "FLAGS.learning_rate = \"0.001,0.0001\"\n",
    "\n",
    "# How many items to train with at once\n",
    "FLAGS.batch_size = 100\n",
    "\n",
    "# Where to save summary logs for TensorBoard.\n",
    "# FLAGS.summaries_dir = '/tmp/retrain_logs'\n",
    "\n",
    "# Directory to write event logs and checkpoint.\n",
    "FLAGS.train_dir = \"training\"\n",
    "\n",
    "# Directory to write converted models to.\n",
    "FLAGS.models_dir = \"models\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7598c217",
   "metadata": {},
   "source": [
    "## 3. Create Keras Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f94f40",
   "metadata": {},
   "source": [
    "Get the model settings as they are required for the preprocessing, training and quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "91604a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_settings = models.prepare_model_settings(\n",
    "    len(data.prepare_words_list(FLAGS.wanted_words.split(\",\"))),\n",
    "    FLAGS.sample_rate,\n",
    "    FLAGS.clip_duration_ms,\n",
    "    FLAGS.window_size_ms,\n",
    "    FLAGS.window_stride_ms,\n",
    "    FLAGS.dct_coefficient_count,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e189906a",
   "metadata": {},
   "source": [
    "Define a model architecture using the Keras API. A predefined model can be found in `models.py`. The model for the final challenge has to be defined in `student/model.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcc0f97-6976-4d93-844d-9247ce51ee8c",
   "metadata": {},
   "source": [
    "The following is just an example on how to define a minimal model architecture for the MicroTVM application:\n",
    "\n",
    "```python\n",
    "def create_micro_kws_xs_model(model_settings):\n",
    "    \"\"\"Builds a model with a single depthwise-convolution layer followed by a single fully-connected layer.\n",
    "    Args:\n",
    "        model_settings: Dict of different settings for model training.\n",
    "    Returns:\n",
    "        tf.keras Model of the 'micro_kws_xs' architecture.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get relevant model setting.\n",
    "    input_frequency_size = model_settings[\"dct_coefficient_count\"]\n",
    "    input_time_size = model_settings[\"spectrogram_length\"]\n",
    "\n",
    "    inputs = tf.keras.Input(shape=(model_settings[\"fingerprint_size\"]), name=\"input\")\n",
    "\n",
    "    # Reshape the flattened input.\n",
    "    x = tf.reshape(inputs, shape=(-1, input_time_size, input_frequency_size, 1))\n",
    "\n",
    "    # First convolution.\n",
    "    x = tf.keras.layers.DepthwiseConv2D(\n",
    "        depth_multiplier=4,\n",
    "        kernel_size=(5, 4),\n",
    "        strides=(2, 2),\n",
    "        padding=\"SAME\",\n",
    "        activation=\"relu\",\n",
    "    )(x)\n",
    "\n",
    "    # Flatten for fully connected layers.\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "\n",
    "    # Output fully connected.\n",
    "    output = tf.keras.layers.Dense(units=model_settings[\"label_count\"], activation=\"softmax\")(x)\n",
    "\n",
    "    return tf.keras.Model(inputs, output, name=FLAGS.model_name)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3278841d",
   "metadata": {},
   "source": [
    "Generate keras model. The `model.summary()` utility provides a way to inspect the layers of Keras model with its shapes and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7ec55b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"micro_model_student\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 1960)]            0         \n",
      "                                                                 \n",
      " tf.reshape_3 (TFOpLambda)   (None, 49, 40, 1)         0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 49, 40, 32)        672       \n",
      "                                                                 \n",
      " depthwise_conv2d_5 (Depthwi  (None, 25, 20, 32)       672       \n",
      " seConv2D)                                                       \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 25, 20, 32)        1056      \n",
      "                                                                 \n",
      " depthwise_conv2d_6 (Depthwi  (None, 25, 20, 128)      1280      \n",
      " seConv2D)                                                       \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 25, 20, 23)        2967      \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 11500)             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 4)                 46004     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 52,651\n",
      "Trainable params: 52,651\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models.get_model(model_settings,\"micro_kws_student\",  model_name=FLAGS.model_name)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6752dda8",
   "metadata": {},
   "source": [
    "## 4. Prepare dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2b321a",
   "metadata": {},
   "source": [
    "While keyword-spotting is quite simple tasks, the preprocessing to generate input features for training is non-trivial. Hence, the implementation of the `Audioprocessor()` class is omited here. If interested, check out the [`data.py`](./data.py) script for more information.\n",
    "\n",
    "One aspect, which is very important here is the `micro=True` option as it ensures that the same preprocessing (conversion of input WAV files to an Image) is applied to the input dataset as used in the mcirocontroller target software."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/almo/speech_dataset\n"
     ]
    }
   ],
   "source": [
    "print(FLAGS.data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "437c0a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_processor = data.AudioProcessor(\n",
    "    data_url=FLAGS.data_url,\n",
    "    data_dir=FLAGS.data_dir,\n",
    "    silence_percentage=FLAGS.silence_percentage,\n",
    "    unknown_percentage=FLAGS.unknown_percentage,\n",
    "    wanted_words=FLAGS.wanted_words.split(\",\"),\n",
    "    validation_percentage=FLAGS.validation_percentage,\n",
    "    testing_percentage=FLAGS.testing_percentage,\n",
    "    model_settings=model_settings,\n",
    "    micro=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed80065",
   "metadata": {},
   "source": [
    "Let's define a helper function to visualize some features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eece066c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_feature(feature):\n",
    "    # Utility to display a given feature from the dataset inside the notebook\n",
    "    feature_data, feature_label = feature\n",
    "\n",
    "    feature_data = feature_data.numpy()\n",
    "    feature_label = feature_label.numpy()\n",
    "\n",
    "    feature_label_str = ([\"silence\", \"unknown\"] + FLAGS.wanted_words.split(\",\"))[feature_label]\n",
    "\n",
    "    feature_reshaped = np.reshape(feature_data, (49, 40)).T\n",
    "\n",
    "    p = plt.imshow(feature_reshaped, cmap=\"gray\", vmin=0, vmax=26)\n",
    "    plt.title(f\"Label: {feature_label_str}\")\n",
    "    plt.xlabel(\"Time [s]\")\n",
    "    plt.ylabel(\"Frequency [ƒ]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c85ca5",
   "metadata": {},
   "source": [
    "Execute the following cell a few times to inpect the generated features for some keywords/labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "61d0d40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-03 22:12:00.797841: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature = (\n",
    "    audio_processor.get_data(audio_processor.Modes.VALIDATION)\n",
    "    .shuffle(100)\n",
    "    .take(1)\n",
    "    .get_single_element()\n",
    ")\n",
    "visualize_feature(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd90760",
   "metadata": {},
   "source": [
    "## 5. Run Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9c5339",
   "metadata": {},
   "source": [
    "Define training procedure using the previously defined parameters.\n",
    "\n",
    "In addition to the training hyperparamerters (training steps, learning rate,...) an optimizer (`Adam`), a loss function (`SparseCategoricalCrossentropy`) as well as a metric is selected for the training and passed to the `model.compile()` method.\n",
    "\n",
    "The actual training happens when `model.fit()` is called. The training progress should be visible on the screen. While multiple epochs (\"one pass over the entire dataset\") are required for the training, the validation accuracy is evaluated every 200 steps and the weights are written to a directory automatically.\n",
    "\n",
    "At the end of the training procedure the final test accuracy of the trained model is printed to the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0f4c2f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, audio_processor):\n",
    "    # We decay learning rate in a constant piecewise way to help learning.\n",
    "    training_steps_list = list(map(int, FLAGS.how_many_training_steps.split(\",\")))\n",
    "    learning_rates_list = list(map(float, FLAGS.learning_rate.split(\",\")))\n",
    "    lr_boundary_list = training_steps_list[:-1]  # Only need the values at which to change lr.\n",
    "    lr_schedule = tf.keras.optimizers.schedules.PiecewiseConstantDecay(\n",
    "        boundaries=lr_boundary_list, values=learning_rates_list\n",
    "    )\n",
    "\n",
    "    # Specify the optimizer configurations.\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "    # Compile the model.\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    # Prepare/split the dataset.\n",
    "    train_data = audio_processor.get_data(\n",
    "        audio_processor.Modes.TRAINING,\n",
    "        FLAGS.background_frequency,\n",
    "        FLAGS.background_volume,\n",
    "        int((FLAGS.time_shift_ms * FLAGS.sample_rate) / 1000),\n",
    "    )\n",
    "    train_data = train_data.repeat().batch(FLAGS.batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    val_data = audio_processor.get_data(audio_processor.Modes.VALIDATION)\n",
    "    val_data = val_data.batch(FLAGS.batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    # We train for a max number of iterations so need to calculate how many 'epochs' this will be.\n",
    "    training_steps_max = np.sum(training_steps_list)\n",
    "    training_epoch_max = int(np.ceil(training_steps_max / FLAGS.eval_step_interval))\n",
    "\n",
    "    # Callbacks.\n",
    "    train_dir = Path(FLAGS.train_dir) / FLAGS.model_name / \"best\"\n",
    "    train_dir.mkdir(parents=True, exist_ok=True)\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=(train_dir / (FLAGS.model_name + \"_{val_accuracy:.3f}_ckpt\")),\n",
    "        save_weights_only=True,\n",
    "        monitor=\"val_accuracy\",\n",
    "        mode=\"max\",\n",
    "        save_best_only=True,\n",
    "    )\n",
    "\n",
    "    # Train the model.\n",
    "    model.fit(\n",
    "        x=train_data,\n",
    "        steps_per_epoch=FLAGS.eval_step_interval,\n",
    "        epochs=training_epoch_max,\n",
    "        validation_data=val_data,\n",
    "        callbacks=[model_checkpoint_callback, *get_student_callbacks()],\n",
    "    )\n",
    "\n",
    "    # Test and save the model.\n",
    "    test_data = audio_processor.get_data(audio_processor.Modes.TESTING)\n",
    "    test_data = test_data.batch(FLAGS.batch_size)\n",
    "\n",
    "    # Evaluate the model performace.\n",
    "    test_loss, test_acc = model.evaluate(x=test_data)\n",
    "    print(f\"Final test accuracy: {test_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48fc3e4",
   "metadata": {},
   "source": [
    "Invoke training procedure (**Warning:** This will take a very long time!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ab42c6a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/38\n",
      "400/400 [==============================] - 12s 28ms/step - loss: 0.5562 - accuracy: 0.7971 - val_loss: 0.2738 - val_accuracy: 0.8996\n",
      "Epoch 2/38\n",
      "400/400 [==============================] - 11s 28ms/step - loss: 0.2874 - accuracy: 0.8963 - val_loss: 0.2405 - val_accuracy: 0.9071\n",
      "Epoch 3/38\n",
      "400/400 [==============================] - 14s 35ms/step - loss: 0.2192 - accuracy: 0.9226 - val_loss: 0.1954 - val_accuracy: 0.9303\n",
      "Epoch 4/38\n",
      "400/400 [==============================] - 14s 35ms/step - loss: 0.2065 - accuracy: 0.9265 - val_loss: 0.2119 - val_accuracy: 0.9328\n",
      "Epoch 5/38\n",
      "400/400 [==============================] - 14s 35ms/step - loss: 0.1898 - accuracy: 0.9333 - val_loss: 0.1543 - val_accuracy: 0.9444\n",
      "Epoch 6/38\n",
      "400/400 [==============================] - 14s 34ms/step - loss: 0.1613 - accuracy: 0.9441 - val_loss: 0.1400 - val_accuracy: 0.9452\n",
      "Epoch 7/38\n",
      "400/400 [==============================] - 14s 35ms/step - loss: 0.1535 - accuracy: 0.9472 - val_loss: 0.1765 - val_accuracy: 0.9328\n",
      "Epoch 8/38\n",
      "400/400 [==============================] - 15s 36ms/step - loss: 0.1635 - accuracy: 0.9443 - val_loss: 0.1484 - val_accuracy: 0.9469\n",
      "Epoch 9/38\n",
      "400/400 [==============================] - 14s 35ms/step - loss: 0.1274 - accuracy: 0.9552 - val_loss: 0.1329 - val_accuracy: 0.9568\n",
      "Epoch 10/38\n",
      "400/400 [==============================] - 14s 35ms/step - loss: 0.1243 - accuracy: 0.9571 - val_loss: 0.1374 - val_accuracy: 0.9535\n",
      "Epoch 11/38\n",
      "400/400 [==============================] - 14s 35ms/step - loss: 0.1097 - accuracy: 0.9616 - val_loss: 0.1176 - val_accuracy: 0.9568\n",
      "Epoch 12/38\n",
      "400/400 [==============================] - 15s 36ms/step - loss: 0.1131 - accuracy: 0.9613 - val_loss: 0.1828 - val_accuracy: 0.9402\n",
      "Epoch 13/38\n",
      "400/400 [==============================] - 14s 36ms/step - loss: 0.1252 - accuracy: 0.9582 - val_loss: 0.1253 - val_accuracy: 0.9552\n",
      "Epoch 14/38\n",
      "400/400 [==============================] - 14s 35ms/step - loss: 0.1104 - accuracy: 0.9617 - val_loss: 0.1260 - val_accuracy: 0.9577\n",
      "Epoch 15/38\n",
      "400/400 [==============================] - 14s 35ms/step - loss: 0.1113 - accuracy: 0.9634 - val_loss: 0.1283 - val_accuracy: 0.9568\n",
      "Epoch 16/38\n",
      "400/400 [==============================] - 14s 35ms/step - loss: 0.0943 - accuracy: 0.9683 - val_loss: 0.1186 - val_accuracy: 0.9635\n",
      "Epoch 17/38\n",
      "400/400 [==============================] - 14s 36ms/step - loss: 0.1156 - accuracy: 0.9606 - val_loss: 0.1121 - val_accuracy: 0.9544\n",
      "Epoch 18/38\n",
      "400/400 [==============================] - 14s 36ms/step - loss: 0.1035 - accuracy: 0.9642 - val_loss: 0.1256 - val_accuracy: 0.9610\n",
      "Epoch 19/38\n",
      "400/400 [==============================] - 14s 35ms/step - loss: 0.1136 - accuracy: 0.9627 - val_loss: 0.1133 - val_accuracy: 0.9643\n",
      "Epoch 20/38\n",
      "400/400 [==============================] - 14s 35ms/step - loss: 0.0908 - accuracy: 0.9697 - val_loss: 0.1931 - val_accuracy: 0.9411\n",
      "Epoch 21/38\n",
      "400/400 [==============================] - 15s 37ms/step - loss: 0.0827 - accuracy: 0.9714 - val_loss: 0.1218 - val_accuracy: 0.9685\n",
      "Epoch 22/38\n",
      "400/400 [==============================] - 15s 37ms/step - loss: 0.1035 - accuracy: 0.9665 - val_loss: 0.1328 - val_accuracy: 0.9560\n",
      "Epoch 23/38\n",
      "400/400 [==============================] - 14s 36ms/step - loss: 0.0771 - accuracy: 0.9736 - val_loss: 0.1198 - val_accuracy: 0.9627\n",
      "Epoch 24/38\n",
      "400/400 [==============================] - 14s 36ms/step - loss: 0.0770 - accuracy: 0.9735 - val_loss: 0.1265 - val_accuracy: 0.9668\n",
      "Epoch 25/38\n",
      "400/400 [==============================] - 15s 37ms/step - loss: 0.0757 - accuracy: 0.9753 - val_loss: 0.1111 - val_accuracy: 0.9651\n",
      "Epoch 26/38\n",
      "400/400 [==============================] - 14s 35ms/step - loss: 0.1313 - accuracy: 0.9570 - val_loss: 0.1270 - val_accuracy: 0.9627\n",
      "Epoch 27/38\n",
      "400/400 [==============================] - 14s 36ms/step - loss: 0.0943 - accuracy: 0.9683 - val_loss: 0.1350 - val_accuracy: 0.9618\n",
      "Epoch 28/38\n",
      "400/400 [==============================] - 14s 35ms/step - loss: 0.0812 - accuracy: 0.9728 - val_loss: 0.1308 - val_accuracy: 0.9602\n",
      "Epoch 29/38\n",
      "400/400 [==============================] - 14s 35ms/step - loss: 0.0648 - accuracy: 0.9791 - val_loss: 0.1096 - val_accuracy: 0.9610\n",
      "Epoch 30/38\n",
      "400/400 [==============================] - 14s 35ms/step - loss: 0.0872 - accuracy: 0.9703 - val_loss: 0.1173 - val_accuracy: 0.9635\n",
      "Epoch 31/38\n",
      "400/400 [==============================] - 14s 35ms/step - loss: 0.0711 - accuracy: 0.9782 - val_loss: 0.1256 - val_accuracy: 0.9585\n",
      "Epoch 32/38\n",
      "400/400 [==============================] - 14s 35ms/step - loss: 0.0624 - accuracy: 0.9796 - val_loss: 0.1215 - val_accuracy: 0.9618\n",
      "Epoch 33/38\n",
      "400/400 [==============================] - 14s 35ms/step - loss: 0.0566 - accuracy: 0.9816 - val_loss: 0.1198 - val_accuracy: 0.9651\n",
      "Epoch 34/38\n",
      "400/400 [==============================] - 14s 35ms/step - loss: 0.0714 - accuracy: 0.9782 - val_loss: 0.1258 - val_accuracy: 0.9610\n",
      "Epoch 35/38\n",
      "400/400 [==============================] - 14s 35ms/step - loss: 0.0533 - accuracy: 0.9836 - val_loss: 0.1178 - val_accuracy: 0.9660\n",
      "Epoch 36/38\n",
      "400/400 [==============================] - 14s 36ms/step - loss: 0.0587 - accuracy: 0.9821 - val_loss: 0.1201 - val_accuracy: 0.9643\n",
      "Epoch 37/38\n",
      "400/400 [==============================] - 14s 36ms/step - loss: 0.0586 - accuracy: 0.9810 - val_loss: 0.1213 - val_accuracy: 0.9668\n",
      "Epoch 38/38\n",
      "400/400 [==============================] - 14s 36ms/step - loss: 0.0462 - accuracy: 0.9851 - val_loss: 0.1226 - val_accuracy: 0.9651\n",
      "13/13 [==============================] - 0s 18ms/step - loss: 0.1645 - accuracy: 0.9620\n",
      "Final test accuracy: 96.20%\n"
     ]
    }
   ],
   "source": [
    "train(model, audio_processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e0cb48",
   "metadata": {},
   "source": [
    "Determine latest checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a6db9771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training/micro_model_student/best/micro_model_student_0.968_ckpt\n"
     ]
    }
   ],
   "source": [
    "latest = tf.train.latest_checkpoint(Path(FLAGS.train_dir) / FLAGS.model_name / \"best\")\n",
    "print(latest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fa0ebd",
   "metadata": {},
   "source": [
    "Pick a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c3d8bfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS.checkpoint = latest  # Feel free to choose a different one!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1638d3ba-8a8f-4532-9ba5-9f1168b6dbdb",
   "metadata": {},
   "source": [
    "**Task:** Update [`student/callbacks.py`](./student/callbacks.py) to define an EarlyStopping (https://keras.io/api/callbacks/early_stopping/) callback with keras which stops the training procedure after 10 or more epochs of neglectible improvement of the `val_loss` quantity. Then rerun the training procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e88bbc",
   "metadata": {},
   "source": [
    "## 6. Test trained TensorFlow model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454e1282",
   "metadata": {},
   "source": [
    "Define test procedure we can use to evaluate our models performance.\n",
    "\n",
    "The used test routines are defined in [`test.py`](./test.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "98d71f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, audio_processor, model_settings):\n",
    "    \"\"\"Calculate accuracy and confusion matrices on validation and test sets.\n",
    "\n",
    "    Model is created and weights loaded from supplied command line arguments.\n",
    "    \"\"\"\n",
    "    model.load_weights(FLAGS.checkpoint).expect_partial()\n",
    "\n",
    "    # Get test data\n",
    "    data = audio_processor.get_data(audio_processor.Modes.TESTING).batch(FLAGS.batch_size)\n",
    "\n",
    "    # Invoke model\n",
    "    predictions = model.predict(data)\n",
    "\n",
    "    # Calculate indices\n",
    "    expected_indices = np.concatenate([y for x, y in data])\n",
    "    predicted_indices = tf.argmax(predictions, axis=1)\n",
    "\n",
    "    print(\"Running testing on test set...\")\n",
    "    accuracy = get_accuracy(expected_indices, predicted_indices)\n",
    "    confusion_matrix = get_confusion_matrix(expected_indices, predicted_indices, model_settings)\n",
    "\n",
    "    # Print accuracy and confusion matrix\n",
    "    print(\n",
    "        f\"test accuracy = {accuracy * 100:.2f}%\"\n",
    "        f\"(N={audio_processor.set_size(audio_processor.Modes.TESTING)})\"\n",
    "    )\n",
    "    print()\n",
    "    print(\"confusion matrix:\")\n",
    "    print(confusion_matrix.numpy())\n",
    "\n",
    "    # Print student metrics\n",
    "    print()\n",
    "    print(\"metrics:\")\n",
    "    words = [\"silence\", \"unknown\"] + FLAGS.wanted_words.split(\",\")\n",
    "    for idx, label in enumerate(words):\n",
    "        data = get_student_metrics(confusion_matrix, idx)\n",
    "\n",
    "        # Filter None values\n",
    "        data = {key: value for key, value in data.items() if value is not None}\n",
    "        if len(data) == 0:\n",
    "            continue\n",
    "        print(f\"  {label}:\")\n",
    "        for key, value in data.items():\n",
    "            if isinstance(value, float):\n",
    "                value = f\"{value:.3f}\"\n",
    "            print(f\"    {key} = {value}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aac3a38",
   "metadata": {},
   "source": [
    "Run test procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4e8a52a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 14ms/step\n",
      "Running testing on test set...\n",
      "test accuracy = 95.06%(N=1236)\n",
      "\n",
      "confusion matrix:\n",
      "[[206   0   0   0]\n",
      " [  0 186   5  15]\n",
      " [  1  13 399   6]\n",
      " [  0  17   4 384]]\n",
      "\n",
      "metrics:\n"
     ]
    }
   ],
   "source": [
    "test(model, audio_processor, model_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c2416a",
   "metadata": {},
   "source": [
    "Confusion matrices are also printed as they provide infomation about how the individual classes have performed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4c68e5-9c78-4630-95e6-e88c6cac4a84",
   "metadata": {},
   "source": [
    "**Task:** After completing the programming tasks in `student/metrics.py` the per-class recall, precision and f1-score will be printed above as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4edd49a",
   "metadata": {},
   "source": [
    "## 7. Quantization and Conversion to TFLite "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0a558a",
   "metadata": {},
   "source": [
    "Define conversion procedure using the `TFLiteConverter` which creates a `.tflite` file which holds the model graph and constant weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8a1e04f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_REP_DATA_SAMPLES = (\n",
    "    100  # Number of representative samples which will be used for the post-training quantization.\n",
    ")\n",
    "\n",
    "\n",
    "def convert(model, audio_processor, checkpoint, quantize, inference_type, tflite_path):\n",
    "    \"\"\"Load our trained floating point model and convert it.\n",
    "    TFLite conversion or post training quantization is performed and the\n",
    "    resulting model is saved as a TFLite file.\n",
    "    We use samples from the validation set to do post training quantization.\n",
    "    Args:\n",
    "        model: The keras model.\n",
    "        audio_processor: Audio processor class object.\n",
    "        checkpoint: Path to training checkpoint to load.\n",
    "        quantize: Whether to quantize the model or convert to fp32 TFLite model.\n",
    "        inference_type: Input/output type of the quantized model.\n",
    "        tflite_path: Output TFLite file save path.\n",
    "    \"\"\"\n",
    "    model.load_weights(checkpoint).expect_partial()\n",
    "\n",
    "    val_data = audio_processor.get_data(audio_processor.Modes.VALIDATION).batch(1)\n",
    "\n",
    "    def _rep_dataset():\n",
    "        \"\"\"Generator function to produce representative dataset.\"\"\"\n",
    "        i = 0\n",
    "        for mfcc, label in val_data:\n",
    "            if i > NUM_REP_DATA_SAMPLES:\n",
    "                break\n",
    "            i += 1\n",
    "            yield [mfcc]\n",
    "\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "    if quantize:\n",
    "        # Quantize model and save to disk.\n",
    "        if inference_type == \"int8\":\n",
    "            converter.inference_input_type = tf.int8\n",
    "            converter.inference_output_type = tf.int8\n",
    "\n",
    "        # Int8 post training quantization needs representative dataset.\n",
    "        converter.representative_dataset = _rep_dataset\n",
    "        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "\n",
    "    tflite_model = converter.convert()\n",
    "    with open(tflite_path, \"wb\") as f:\n",
    "        f.write(tflite_model)\n",
    "    print(\"{} model saved to {}.\".format(\"Quantized\" if quantize else \"Converted\", tflite_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f18f12",
   "metadata": {},
   "source": [
    "Invoke the previously defined conversion routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "674c5c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpp8qu5jkg/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpp8qu5jkg/assets\n",
      "2023-11-03 22:20:56.676859: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2023-11-03 22:20:56.676882: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted model saved to models/micro_model_student_yesno.tflite.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp1xi5tqr5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp1xi5tqr5/assets\n",
      "/home/almo/Programs/anaconda3/envs/esd/lib/python3.8/site-packages/tensorflow/lite/python/convert.py:765: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n",
      "2023-11-03 22:20:57.499120: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2023-11-03 22:20:57.499137: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model saved to models/micro_model_student_yesno_quantized.tflite.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fully_quantize: 0, inference_type: 6, input_inference_type: INT8, output_inference_type: INT8\n"
     ]
    }
   ],
   "source": [
    "Path(FLAGS.models_dir).mkdir(exist_ok=True)\n",
    "keywords_str = FLAGS.wanted_words.replace(\",\", \"\")\n",
    "tflite_path_quantized = (\n",
    "    Path(FLAGS.models_dir) / f\"{FLAGS.model_name}_{keywords_str}_quantized.tflite\"\n",
    ")\n",
    "tflite_path = Path(FLAGS.models_dir) / f\"{FLAGS.model_name}_{keywords_str}.tflite\"\n",
    "\n",
    "# Load floating point model from checkpoint and convert it.\n",
    "convert(model, audio_processor, FLAGS.checkpoint, False, \"fp32\", tflite_path)\n",
    "\n",
    "# Quantize model from checkpoint and convert it.\n",
    "convert(model, audio_processor, FLAGS.checkpoint, True, \"int8\", tflite_path_quantized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f553a6c0",
   "metadata": {},
   "source": [
    "## 8. Test Converted TFLite Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26094f4",
   "metadata": {},
   "source": [
    "Test the newly converted model on the test set.\n",
    "\n",
    "The `tflite_test` function is defined in [`test_tflite.py`](./test_tflite.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966ea902",
   "metadata": {},
   "source": [
    "**Floating Point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "60725811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running testing on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy = 95.06%(N=1236)\n",
      "\n",
      "confusion matrix:\n",
      "[[206   0   0   0]\n",
      " [  0 186   5  15]\n",
      " [  1  13 399   6]\n",
      " [  0  17   4 384]]\n"
     ]
    }
   ],
   "source": [
    "_ = tflite_test(model_settings, audio_processor, str(tflite_path), mode=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9106b4",
   "metadata": {},
   "source": [
    "**Quantized**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9bb58095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running testing on test set...\n",
      "test accuracy = 94.66%(N=1236)\n",
      "\n",
      "confusion matrix:\n",
      "[[206   0   0   0]\n",
      " [  0 187   4  15]\n",
      " [  1  14 398   6]\n",
      " [  0  21   5 379]]\n"
     ]
    }
   ],
   "source": [
    "_ = tflite_test(model_settings, audio_processor, str(tflite_path_quantized), mode=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29377504",
   "metadata": {},
   "source": [
    "Interestingly the accuracy of the quantized model can be even better than the floating point one. This behavior is a consequence of the quantization procedure the TFLite which requires a `representative_dataset` to partially re-train some weights during the quantization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870b568b",
   "metadata": {},
   "source": [
    "## 9. Visualize TFLite Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9117c52",
   "metadata": {},
   "source": [
    "**Figure 1:** Example TFLite KWS Model\n",
    "<img src=\"resources/micro_kws_xs_yesno_quantized.png\" alt=\"Netron graph\" width=\"150\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3faa45e",
   "metadata": {},
   "source": [
    "Use the following links to download the generated `.tflite` files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb1ef17",
   "metadata": {},
   "source": [
    "**Floating Point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "798f3616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='models/micro_model_student_yesno.tflite' target='_blank'>models/micro_model_student_yesno.tflite</a><br>"
      ],
      "text/plain": [
       "/media/almo/Windows/Users/Thien/Documents/MSNE/WS23_ESD4ML/micro-kws/1_train/models/micro_model_student_yesno.tflite"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FileLink(tflite_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c9a8be",
   "metadata": {},
   "source": [
    "**Quantized**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5a700cbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='models/micro_model_student_yesno_quantized.tflite' target='_blank'>models/micro_model_student_yesno_quantized.tflite</a><br>"
      ],
      "text/plain": [
       "/media/almo/Windows/Users/Thien/Documents/MSNE/WS23_ESD4ML/micro-kws/1_train/models/micro_model_student_yesno_quantized.tflite"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FileLink(tflite_path_quantized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac432529",
   "metadata": {},
   "source": [
    "Use the web application https://netron.app/ to generate a graph representation of the converted model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a61ea1",
   "metadata": {},
   "source": [
    "## 10. Performance and Memory Estimations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dad9be",
   "metadata": {},
   "source": [
    "In this section you will learn how to estimate the complexity of a given model architecture to evaluate if it is suitable to be deployed on a very constrained embedded device. We will also make use of some simplifications to make our life easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5d71f8bc-4e7f-4567-b30b-edec0183b0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = load_model(tflite_path_quantized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e0977c",
   "metadata": {},
   "source": [
    "### 10.1 ROM Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e0aaf9",
   "metadata": {},
   "source": [
    "Estimating the ROM usage of model graph is non-trivial if considering all types of data which contributes to the ROM footprint e.g.\n",
    "\n",
    "- Code size of the implementation of neural network kernels\n",
    "- ROM overhead of the used inference engine/runtime\n",
    "- The constant weights used by the kernels/operators\n",
    "- Driver code for interfacing with peripherals such as sensors or a serial communication port\n",
    "- If not running bare-metal: Additional ROM usage depending on the used operating system or RTOS\n",
    "\n",
    "In addition the final ROM size depends on further properties such as the used compiler flags (optimization level) and certain memory alignment requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1baccdb2",
   "metadata": {},
   "source": [
    "Lets consider the previously trained `micro_kws_xs_yesno_quantized.tflite` model: It's file size is $10704$ bytes. However the TFLite Flatbuffer format also stores a compat representation of the models tensors (name, shape, type,...) and operators (inputs, outputs, parameters) and some metadata alongside with the model weights which contribute to the biggest part of the file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08f6381",
   "metadata": {},
   "source": [
    "In the following we will only consider the actual weights used by the model, hence we can ignore any implementation specific overheads.\n",
    "\n",
    "The used tensor datatypes (specifically `float32`, `int8` or `int32`) per operator have to be investigated to calculate the total amount of RAM required by the model weights. The https://netron.app/ can be used for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82c6dca",
   "metadata": {},
   "source": [
    "**Task:** Derive a formula to estimate the memory requirement to store all constant weights of the quantized model in ROM considering the used data types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e50185e-54c1-400d-9910-8fa66285e6dc",
   "metadata": {},
   "source": [
    "**Task:** Update the `estimate_rom` utility in [`student/estimate.py`](./student/estimate.py) to use your derived formula."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e48311-2327-4b13-bfcf-701af821c8c4",
   "metadata": {},
   "source": [
    "Execute the following cell to test run your implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a38b7ec2-67af-46e5-99f0-75452c30f64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated ROM Usage: 0.000 KiB\n"
     ]
    }
   ],
   "source": [
    "estimated_rom = estimate_model_rom(m)\n",
    "print(f\"Estimated ROM Usage: {estimated_rom/1e3:.3f} KiB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6456a0",
   "metadata": {},
   "source": [
    "### 10.2 RAM Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a46407",
   "metadata": {},
   "source": [
    "Investigating the RAM usage of a given model involves similar challenges as the ROM-estimations.\n",
    "\n",
    "RAM requirements can vary a lot with the chosen model architecture and deployment flow. The largest contribution to the RAM footprint are often intermediate tensor buffers (activations) but also temporary scratchpad memory required by certain kernel implementations.\n",
    "\n",
    "Optionally memory planning can be used to reduce the RAM usage by analysing the lifetime of certain input and output buffers. This process can happen during runtime (online) or statically (offline) depending on deployment approach.\n",
    "\n",
    "Again additional application-specifc overheads might also be non-negligible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16f7b2c",
   "metadata": {},
   "source": [
    "**Task:** Derive formulars for estimating the dynamic memory requirement of the quantized model based on the TFLite graph only considering intermediate tensor buffers stored in RAM for optimal memory-planning.\n",
    "\n",
    "*Assumptions:*\n",
    "- Neither branches nor nodes with multiple inputs/outputs extist in the trained model.\n",
    "- Assume that the graph is processed in a linear way so that at most 2 buffers will be used at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fb4d18-4d99-4b68-bd80-704b262787b3",
   "metadata": {},
   "source": [
    "**Task:** Update the `estimate_ram` utility in [`student/estimate.py`](./student/estimate.pyestimate_rom) to use your derived formulas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4627fb5-381a-469a-a6a0-d12e8cea6e86",
   "metadata": {},
   "source": [
    "Execute the following cell to test run your implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "431b7860-0d60-48f3-8f49-31d2c9eee65d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated RAM Usage: 0.000 KiB\n"
     ]
    }
   ],
   "source": [
    "estimated_ram = estimate_model_ram(m)\n",
    "print(f\"Estimated RAM Usage: {estimated_ram/1e3:.3f} KiB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd12112",
   "metadata": {},
   "source": [
    "### 10.3 Number of MAC Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8362b66",
   "metadata": {},
   "source": [
    "In this section the compute demand of a given TFLite model should be estimated.\n",
    "\n",
    "As a first simplification we will only consider the operation which will have the biggest impact on the actual inference time: Multiply-Add (MAC)\n",
    "\n",
    "These operations can be found in Dense (FullyConnected), and convolutional layers. Thus other operations (here: Reshape, Flatten as well as activation functions) can be neglected for the following task.\n",
    "\n",
    "First, a formular to describe the number of MAC operations of the three major types of with repect to the given tensor dimensions and parameters.\n",
    "\n",
    "**Example (Dense/FullyConnected):**\n",
    "\n",
    "  Assume: $h_{out}=h_{in}$, $w_{out}=w_{filter}$\n",
    "\n",
    "  $$num_{mac} = h_{out} \\cdot w_{out} \\cdot h_{filter}$$\n",
    "  \n",
    "  For the example keras model: $1 \\cdot 4 \\cdot 2000 \\approx 8k \\mathrm{MACs}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdbe0b0",
   "metadata": {},
   "source": [
    "**Task:** Estimate the number of Multiply-Add operations used in the quantized model (see Figure 1) by deriving a formula for `num_mac` in a (depthwise) convolutional layer with respect to $$h_{kernel}, w_{kernel}, c_{in}, c_{out}, h_{out}, w_{out}$$ and (if applicable) $$depth\\_multiplier,h_{stride}, w_{stride}$$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54110cd3-7ee6-4259-8647-a6d2b0febf81",
   "metadata": {},
   "source": [
    "**Task:** Update the `estimate_fully_connected`, `estimate_conv2d_macs` and `estimate_depthwise_conv2d_macs` in [`student/estimate.py`](./student/estimate.py) to use your derived formulas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5362bece-e991-4e96-a9ee-9275f01091a6",
   "metadata": {},
   "source": [
    "Execute the following cell to test run your implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d44a5976-f5c7-4a7e-b5ef-231697bcb9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated MACs: 0\n"
     ]
    }
   ],
   "source": [
    "estimated_macs = estimate_model_macs(m)\n",
    "print(f\"Estimated MACs: {estimated_macs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79185f20",
   "metadata": {},
   "source": [
    "## 11. Final challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93479ba",
   "metadata": {},
   "source": [
    "**Task:** To get bonus credits in the lab you have to design a model architecture for the keyword-spotting task which satisfies each of the following constraints:\n",
    "\n",
    "See `Lab 1 Manual`!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1cbedf-bd67-4f59-8d55-19ffcd2588e4",
   "metadata": {},
   "source": [
    "## 12. Lab 1 Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443bb95b-0111-441b-bec8-0323caf86125",
   "metadata": {},
   "source": [
    "The following cell can be executed to run some basic tests on your code. The converage of these unit tests is far away from complete and 100% successful tests to not imply a correct solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "18512853-75f2-44a3-9a5d-53197051e436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.8.18, pytest-7.4.3, pluggy-1.3.0\n",
      "rootdir: /media/almo/Windows/Users/Thien/Documents/MSNE/WS23_ESD4ML/micro-kws/1_train\n",
      "configfile: pytest.ini\n",
      "plugins: anyio-4.0.0\n",
      "collected 7 items                                                              \u001b[0m\u001b[1m\n",
      "\n",
      "tests/test_callbacks.py \u001b[31mF\u001b[0m\u001b[31m                                                [ 14%]\u001b[0m\n",
      "tests/test_estimate.py \u001b[31mF\u001b[0m\u001b[31mF\u001b[0m\u001b[31mF\u001b[0m\u001b[31m                                               [ 57%]\u001b[0m\n",
      "tests/test_metrics.py \u001b[31mF\u001b[0m\u001b[31mF\u001b[0m\u001b[31mF\u001b[0m\u001b[31m                                                [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m________________________ test_callbacks_early_stopping _________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_callbacks_early_stopping\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      "        callback = get_early_stopping_callback()\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m callback\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       assert None\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_callbacks.py\u001b[0m:9: AssertionError\n",
      "\u001b[31m\u001b[1m______________________________ test_estimate_macs ______________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_estimate_macs\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# Trivial cases\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m estimate_depthwise_conv2d_macs([\u001b[94m1\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m], [\u001b[94m1\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m], [\u001b[94m1\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m], \u001b[94m1\u001b[39;49;00m) == \u001b[94m1\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       assert 0 == 1\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where 0 = estimate_depthwise_conv2d_macs([1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], 1)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_estimate.py\u001b[0m:59: AssertionError\n",
      "\u001b[31m\u001b[1m______________________________ test_estimate_rom _______________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_estimate_rom\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# Trivial cases\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m estimate_rom([MyTensor(\u001b[94m0\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mfoo\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m1\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m], \u001b[33m\"\u001b[39;49;00m\u001b[33mint8\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m)]) == \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# Single layer\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m estimate_rom([INPUT, WEIGHTS_1, BIAS_1, INTERMEDIATE_1]) == \u001b[94m160\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: assert 0 == 160\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where 0 = estimate_rom([MyTensor(idx=0, name='x0', shape=[1, 32, 32, 4], dtype='int8', is_const=False), MyTensor(idx=1, name='w1', shape=[1, ...pe=[8], dtype='int32', is_const=True), MyTensor(idx=3, name='x1', shape=[1, 32, 32, 32], dtype='int8', is_const=False)])\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_estimate.py\u001b[0m:83: AssertionError\n",
      "\u001b[31m\u001b[1m______________________________ test_estimate_ram _______________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_estimate_ram\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# Trvial cases\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[94massert\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
      "            estimate_ram(\u001b[90m\u001b[39;49;00m\n",
      "                [MyTensor(\u001b[94m0\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mfoo\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m1\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m], \u001b[33m\"\u001b[39;49;00m\u001b[33mint8\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[94mTrue\u001b[39;49;00m)], [MyLayer(\u001b[94m0\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mbar\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94m0\u001b[39;49;00m], [\u001b[94m0\u001b[39;49;00m])]\u001b[90m\u001b[39;49;00m\n",
      "            )\u001b[90m\u001b[39;49;00m\n",
      "            == \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        )\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# Single layer\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# assert estimate_ram(ALL_TENSORS, [DEPTHWISE_CONV2D]) == (9216, 9216)\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# assert estimate_ram(ALL_TENSORS, [CONV2D]) == (123, 456)\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# assert estimate_ram(ALL_TENSORS, [RESHAPE]) == (123, 456)\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# assert estimate_ram(ALL_TENSORS, [FULLY_CONNECTED]) == (123, 456)\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "    \u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# Multi layer\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m estimate_ram(ALL_TENSORS, ALL_LAYERS) == \u001b[94m36864\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       AssertionError: assert 0 == 36864\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where 0 = estimate_ram([MyTensor(idx=0, name='x0', shape=[1, 32, 32, 4], dtype='int8', is_const=False), MyTensor(idx=1, name='w1', shape=[1, ...16, 3, 3, 32], dtype='int8', is_const=True), MyTensor(idx=5, name='b2', shape=[16], dtype='int32', is_const=True), ...], [MyLayer(idx=0, name='DepthwiseConv2D', inputs=[0, 1, 2], outputs=[3]), MyLayer(idx=1, name='Conv2D', inputs=[3, 4, 5]...2, name='Reshape', inputs=[6, 7], outputs=[8]), MyLayer(idx=3, name='FullyConnected', inputs=[8, 9, 10], outputs=[11])])\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_estimate.py\u001b[0m:108: AssertionError\n",
      "\u001b[31m\u001b[1m____________________________ test_metrics_precision ____________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_metrics_precision\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# matrix 1\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_almost_equal(precision(MATRIX_1, \u001b[94m0\u001b[39;49;00m), \u001b[94m0.5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       TypeError: unsupported operand type(s) for -: 'float' and 'NoneType'\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_metrics.py\u001b[0m:21: TypeError\n",
      "\u001b[31m\u001b[1m_____________________________ test_metrics_recall ______________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_metrics_recall\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# matrix 1\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_almost_equal(recall(MATRIX_1, \u001b[94m0\u001b[39;49;00m), \u001b[94m0.5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       TypeError: unsupported operand type(s) for -: 'float' and 'NoneType'\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_metrics.py\u001b[0m:33: TypeError\n",
      "\u001b[31m\u001b[1m____________________________ test_metrics_f1_score _____________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_metrics_f1_score\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      "        \u001b[90m# matrix 1\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       np.testing.assert_almost_equal(f1_score(MATRIX_1, \u001b[94m0\u001b[39;49;00m), \u001b[94m0.5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       TypeError: unsupported operand type(s) for -: 'float' and 'NoneType'\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtests/test_metrics.py\u001b[0m:45: TypeError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m tests/test_callbacks.py::\u001b[1mtest_callbacks_early_stopping\u001b[0m - assert None\n",
      "\u001b[31mFAILED\u001b[0m tests/test_estimate.py::\u001b[1mtest_estimate_macs\u001b[0m - assert 0 == 1\n",
      "\u001b[31mFAILED\u001b[0m tests/test_estimate.py::\u001b[1mtest_estimate_rom\u001b[0m - AssertionError: assert 0 == 160\n",
      "\u001b[31mFAILED\u001b[0m tests/test_estimate.py::\u001b[1mtest_estimate_ram\u001b[0m - AssertionError: assert 0 == 36864\n",
      "\u001b[31mFAILED\u001b[0m tests/test_metrics.py::\u001b[1mtest_metrics_precision\u001b[0m - TypeError: unsupported operand type(s) for -: 'float' and 'NoneType'\n",
      "\u001b[31mFAILED\u001b[0m tests/test_metrics.py::\u001b[1mtest_metrics_recall\u001b[0m - TypeError: unsupported operand type(s) for -: 'float' and 'NoneType'\n",
      "\u001b[31mFAILED\u001b[0m tests/test_metrics.py::\u001b[1mtest_metrics_f1_score\u001b[0m - TypeError: unsupported operand type(s) for -: 'float' and 'NoneType'\n",
      "\u001b[31m============================== \u001b[31m\u001b[1m7 failed\u001b[0m\u001b[31m in 1.23s\u001b[0m\u001b[31m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest tests/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72400c88-5031-4451-b97a-50ed129a87a0",
   "metadata": {},
   "source": [
    "After completing the lab execises, the following cell can be executed to generate the ZIP file containing the expected files. This script also runs some basic checks to make sure that nothing is missing in your submission. The `submission.zip` file has to be uploaded to Moodle before the deadline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "55ee8cf5-95ec-40d7-8ffe-b406fb22ae11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"submit.py\", line 108, in <module>\n",
      "    main()\n",
      "  File \"submit.py\", line 96, in main\n",
      "    files = gather_files(directory)\n",
      "  File \"submit.py\", line 64, in gather_files\n",
      "    assert model_path.is_file(), f\"{model_path} file does not exist\"\n",
      "AssertionError: models/micro_kws_student_onleftrightyesoffdownnoup.tflite file does not exist\n"
     ]
    }
   ],
   "source": [
    "!python submit.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c163b7-f68b-414a-8a26-33ca25045fe2",
   "metadata": {},
   "source": [
    "This is the end of the Notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
